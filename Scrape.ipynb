{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests.exceptions import RequestException\n",
    "from contextlib import closing\n",
    "import re\n",
    "from lxml import html\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import Algorithmia\n",
    "# Imports the Google Cloud client library\n",
    "from google.cloud import language\n",
    "from google.cloud.language import enums\n",
    "from google.cloud.language import types\n",
    "\n",
    "import os\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = r\"./climate_sentiment_key.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Algorithmia.client('sim0ds3RRSQiux4Q9vb47cqyuHe1')\n",
    "algo = client.algo('nlp/SentimentAnalysis/1.0.5')\n",
    "def get_algorithmia_sentiment(text):\n",
    "    algo_in = {\n",
    "          \"document\": text\n",
    "        }\n",
    "    return algo.pipe(algo_in).result[0]['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiates a client\n",
    "client = language.LanguageServiceClient()\n",
    "\n",
    "def get_gcp_sentiment(text):\n",
    "    document = types.Document(\n",
    "        content=text,\n",
    "        type=enums.Document.Type.PLAIN_TEXT)\n",
    "    # Detects the sentiment of the text\n",
    "    sentiment = client.analyze_sentiment(document=document).document_sentiment\n",
    "    return sentiment.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_from_url(url):\n",
    "    date_search = r\"(\\d+)\\/(\\d+)\\/(\\d+)\"\n",
    "    return datetime(*[int(x) for x in re.search(date_search, url).groups()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_sentiment(sentiments):\n",
    "    return sum([x[0] for x in res]) / len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment_by_sentences(url):\n",
    "    # download HTML from simplified page\n",
    "    base_url = \"https://www.textise.net/showText.aspx?strURL=\"\n",
    "    print(\"Downloading HTML page...\")\n",
    "    page = requests.get(base_url + url.replace(\":\", \"%253A\"))\n",
    "    tree = html.fromstring(page.content)\n",
    "    print(\"Finished downloading. Parsing...\")\n",
    "    \n",
    "    divs = []\n",
    "    for div in tree.xpath('//div/text()'):\n",
    "        divs.append(div.rstrip())\n",
    "    # delete filler text\n",
    "    delete = [\"Espa√±ol\", 'Set edition preference:', u'\\xa0 -', \"International  Edition\", \"International Edition\", \"Find us on\", \"Read More\", 'U.S.', \"Arabic\", 'International', 'Here are some options:', \"Switzerland\", \" FOLLOW CNN BUSINESS \"]\n",
    "    divs = filter(lambda x: re.sub(' \\d+ of \\d+', '', x), divs)\n",
    "    divs = filter(lambda x: x != [], divs)\n",
    "    divs = filter(lambda x: x != \"<div>\", divs)\n",
    "    divs = filter(lambda x: x != \"</div>\", divs)\n",
    "    divs = filter(lambda x: x != \"\\xa0\", divs)\n",
    "    divs = filter(lambda x: x != \".\", divs)\n",
    "    divs = filter(lambda x: x != \" |\", divs)\n",
    "    divs = filter(lambda x: x != \"\", divs)\n",
    "    divs = filter(lambda x: \"/>\" not in x, divs)\n",
    "    divs = filter(lambda x: \"</\" not in x, divs)\n",
    "    divs = filter(lambda x: \"http\" not in x, divs)\n",
    "    divs = filter(lambda x: \"Image:\" not in x, divs)\n",
    "    divs = filter(lambda x: u\"\\xa0\" not in x, divs)\n",
    "    divs = filter(lambda x: \"Facebook Messenger\" not in x, divs)\n",
    "    divs = filter(lambda x: \"Hide Caption\" not in x, divs)\n",
    "    divs = filter(lambda x: \"MUST WATCH\" not in x, divs)\n",
    "    divs = filter(lambda x: x not in delete, divs)\n",
    "    divs = \"\".join(list(divs)[1:-4])\n",
    "    sentences = [x.rstrip() for x in divs.split(\".\")]\n",
    "    sentiments = []\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        #sentiments.append(get_algorithmia_sentiment(sentence))\n",
    "        sentiments.append((get_gcp_sentiment(sentence), sentence))\n",
    "        print(\"Finished analyzing sentence {} of {}\".format(i + 1, len(sentences)))\n",
    "        \n",
    "    return sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
